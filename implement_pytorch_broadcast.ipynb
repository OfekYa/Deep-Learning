{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNq60OCp2Ct2ZvoVZd9JwZ8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OfekYa/Deep-Learning/blob/main/implement_pytorch_broadcast.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implement the broadcasting functionality of PyTorch"
      ],
      "metadata": {
        "id": "r8WlZ5xLII6h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "r6_3cxz4DboI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 1_a**"
      ],
      "metadata": {
        "id": "tv6OG2bwE-e3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XLTuv1MQnZc0"
      },
      "outputs": [],
      "source": [
        "\n",
        "def expand_as(tensor_A, tensor_B):\n",
        "\n",
        "    tensor_B[tensor_B != 0] = 0 # Redefine the B values as 0's. Keeping the original B size.\n",
        "    tensor_C = tensor_A.clone()  # Duplication (שכפול) of A in order not to change the size of A.\n",
        "                                 # (according to the requirement of the exercise)\n",
        "\n",
        "    A_shape = tensor_A.shape  # Save the dimensions of A\n",
        "    B_shape = tensor_B.shape  # Save the dimensions of B\n",
        "\n",
        "    A_current_dim = len(A_shape) - 1  # The rightmost dimension of A\n",
        "    B_current_dim = len(B_shape) - 1  # The rightmost dimension of B\n",
        "\n",
        "    if A_current_dim > B_current_dim:  #  If tensor B has fewer dimensions than tensor A, then A cannot be broadcast to B shape.\n",
        "        raise Exception('tensor ' + str(tensor_A) + ' cannot broadcast to tensor shape: ' + str(tensor_B.shape))\n",
        "\n",
        "    while B_current_dim >= 0:\n",
        "\n",
        "        ''' If tensor A has fewer dimensions than tensor B, then we add an extra dimension at\n",
        "            the beginning of A until the number of dimensions in both tensors is equal. (p.21 [2.a] learning guide) '''\n",
        "        if A_current_dim < 0:\n",
        "            tensor_C.unsqueeze_(0) # Turn an N dim tensor into an (N+1) dim tensor by adding an extra dimension.\n",
        "            A_current_dim += 1\n",
        "            A_shape = tensor_C.shape # C has increased by one dimension, so we need to update the shape.\n",
        "\n",
        "        '''  If the current dimension size of A and B are different, \n",
        "              and A is also different from 1, then A cannot be broadcast to B shape. (p.21 [1.a] learning guide)'''\n",
        "        if (B_shape[B_current_dim] != A_shape[A_current_dim] and A_shape[A_current_dim] != 1):\n",
        "            raise Exception('tensor ' + str(tensor_A) + ' cannot broadcast to tensor shape: ' + str(tensor_B.shape))\n",
        "\n",
        "        A_current_dim -= 1  # Move one dimension to the left in each of the tensors\n",
        "        B_current_dim -= 1\n",
        "\n",
        "    tensor_C = tensor_A + tensor_B  # We will receive the broadcasting of A to the shape of B.\n",
        "    return tensor_C"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 1_b**"
      ],
      "metadata": {
        "id": "6Dt7X3tAFTu9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def broadcastible_together(tensor_A, tensor_B):\n",
        "\n",
        "    A_shape = tensor_A.shape   # Save the dimensions of A\n",
        "    B_shape = tensor_B.shape   # Save the dimensions of B\n",
        "\n",
        "    A_current_dim = len(A_shape) - 1  # The rightmost dimension of A\n",
        "    B_current_dim = len(B_shape) - 1  # The rightmost dimension of B\n",
        "\n",
        "    \"\"\" Go through all the dimensions of at least one of the tensors and check the conditions for broadcasting.\n",
        "        (p.21 [1->a,b,c] learning guide) \"\"\"\n",
        "    while A_current_dim >= 0 and B_current_dim >= 0:\n",
        "        if A_shape[A_current_dim] != B_shape[B_current_dim]:\n",
        "            if A_shape[A_current_dim] != 1 and B_shape[B_current_dim] != 1:\n",
        "                return False  # Unable to broadcasting.\n",
        "\n",
        "        A_current_dim -= 1  # Move one dimension to the left in each of the tensors.\n",
        "        B_current_dim -= 1\n",
        "\n",
        "    return True, (tensor_A + tensor_B).size()  # The size the tensors will be broadcasting to.\n"
      ],
      "metadata": {
        "id": "WYGZRSGfEouD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 1_c**"
      ],
      "metadata": {
        "id": "cSojSlmwFYB7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def broadcast_tensors(tensor_A, tensor_B):\n",
        "\n",
        "    broadcastible, size = broadcastible_together(tensor_A, tensor_B) # check if A and B can broadcast together.\n",
        "    if broadcastible:\n",
        "        tmp_tensor = torch.zeros(size) # Tensor of 0's in size which the tensors A and B will be broadcasting to.\n",
        "        tensor_A = expand_as(tensor_A, tmp_tensor)  # broadcast A\n",
        "        tensor_B = expand_as(tensor_B, tmp_tensor)  # broadcast B\n",
        "        return tensor_A, tensor_B\n",
        "\n"
      ],
      "metadata": {
        "id": "FmVDh2rbEo53"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 1_d**"
      ],
      "metadata": {
        "id": "kUtg1FaVFbgd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def compare_my_expand_as_to_pytorch(test_cases):\n",
        "\n",
        "    for i in range(len(test_cases)):\n",
        "\n",
        "        A = test_cases[i][0]\n",
        "        B = test_cases[i][1]\n",
        "        expected_error, threw_error = False, False\n",
        "\n",
        "        try:\n",
        "            expected_tensor = A.expand_as(B)\n",
        "        except:\n",
        "            expected_error = True\n",
        "        try:\n",
        "            actual_tensor = expand_as(A, B)\n",
        "        except:\n",
        "            threw_error = True\n",
        "        if expected_error != threw_error:\n",
        "            print(\"FAILED: EXCEPTION: expected_error != threw_error\")\n",
        "\n",
        "        elif threw_error:\n",
        "            print(\"SUCCESS: both return exception\")\n",
        "\n",
        "        elif not torch.equal(expected_tensor, actual_tensor):\n",
        "            print(\"FAILED: EXCEPTION: expected tensor!= my tensor\")\n",
        "\n",
        "        else:\n",
        "            print(\"SUCCESS: both return the same tensor\")\n",
        "##############################################################################################################\n",
        "\n",
        "def compare_my_broadcastible_together_to_pytorch(test_cases):\n",
        "\n",
        "    for i in range(len(test_cases)):\n",
        "\n",
        "        A = test_cases[i][0]\n",
        "        B = test_cases[i][1]\n",
        "\n",
        "\n",
        "        expected_result =  False\n",
        "\n",
        "        try:\n",
        "            expected_result = True, torch.broadcast_tensors(A, B)[0].shape\n",
        "\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        try:\n",
        "            actual_result = broadcastible_together(A, B)\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        if type(expected_result) and type(actual_result) is bool: # התוצאות יהיו או ערכי TUPLE או ערכי BOOL:FALSE\n",
        "            print(\"SUCCESS: both return the same bool (FALSE)\")\n",
        "\n",
        "        elif expected_result[1] != actual_result[1]:\n",
        "            print(\"FAILED: EXCEPTION: expected_result[1] != actual_result[1]\")\n",
        "\n",
        "        else:\n",
        "            print(\"SUCCESS: both return the same result\")\n",
        "\n",
        "\n",
        "##############################################################################################################\n",
        "\n",
        "\n",
        "def compare_my_broadcast_tensors_to_pytorch(test_cases):\n",
        "    for i in range(len(test_cases)):\n",
        "\n",
        "        A = test_cases[i][0]\n",
        "        B = test_cases[i][1]\n",
        "\n",
        "        expected_error, threw_error = False, False\n",
        "\n",
        "        try:\n",
        "            expected_result = torch.broadcast_tensors(A, B)\n",
        "        except:\n",
        "            expected_error = True\n",
        "\n",
        "        try:\n",
        "            actual_result = broadcast_tensors(A, B)\n",
        "        except:\n",
        "            threw_error = True\n",
        "\n",
        "        if expected_error != threw_error:\n",
        "            print(\"FAILED: EXCEPTION: expected_error != threw_error\")\n",
        "        elif threw_error:\n",
        "            print(\"SUCCESS: both return exception\")\n",
        "\n",
        "        expected_a, expected_b = expected_result\n",
        "        actual_a, actual_b = actual_result\n",
        "        if not torch.equal(expected_a, actual_a):\n",
        "            print(\"FAILED: bad result for A\")\n",
        "\n",
        "        if not torch.equal(expected_b, actual_b):\n",
        "            print(\"FAILED: bad result for B\")\n",
        "\n",
        "        print(\"SUCCESS: both return the same result\")\n",
        "\n"
      ],
      "metadata": {
        "id": "Ok2LWafUEpGO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "test_cases = [\n",
        "    [torch.tensor([4, 4]), torch.tensor([2])],\n",
        "    [torch.tensor([[1, 6, 7], [1, 6, 7]]), torch.tensor([2])],\n",
        "    [torch.tensor([1, 2]), torch.tensor([[2, 3, 4], [5, 6, 7]])],\n",
        "    [torch.tensor([1, 2, 3]), torch.tensor([[2, 3, 4], [5, 6, 7]])],\n",
        "    [torch.tensor([[1, 2, 3]]), torch.tensor([[2, 3, 4], [5, 6, 7]])],\n",
        "\n",
        "    [torch.tensor([[[1, 2, 3]]]), torch.tensor([[2, 3, 4], [5, 6, 7]])],\n",
        "    [torch.arange(10 ** 4).reshape(10, 10, 10, 1, 10), torch.arange(10 ** 5).view(10, 10, 10, 10, 10)],\n",
        "    [torch.arange(10 ** 3).reshape(10, 1, 10, 1, 10), torch.arange(10 ** 4).view(10, 10, 10, 10)],\n",
        "    [torch.arange(10 ** 3).reshape(10, 10, 1, 10), torch.arange(10 ** 3).view(10, 10, 10)],\n",
        "    [torch.arange(10 ** 2).reshape(10, 1, 1, 10), torch.arange(10 ** 5).view(10, 10, 10, 10, 10)],\n",
        "\n",
        "    [torch.arange(10 ** 2).reshape(10, 10), torch.arange(10 ** 5).view(10, 10, 10, 10, 10)]\n",
        "]\n",
        "print(\"TEST: expand_as\\n\")\n",
        "compare_my_expand_as_to_pytorch(test_cases)\n",
        "print(\"\\n\\nTEST: broadcastible_together\\n\")\n",
        "compare_my_broadcastible_together_to_pytorch(test_cases)\n",
        "print(\"\\n\\nTEST: broadcast_together\\n\")\n",
        "compare_my_broadcast_tensors_to_pytorch(test_cases)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x_I2GvR2E2yQ",
        "outputId": "bba4d758-a80b-4b37-ba1f-fb4de55dfb82"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TEST: expand_as\n",
            "\n",
            "SUCCESS: both return exception\n",
            "SUCCESS: both return exception\n",
            "SUCCESS: both return exception\n",
            "SUCCESS: both return the same tensor\n",
            "SUCCESS: both return the same tensor\n",
            "SUCCESS: both return exception\n",
            "SUCCESS: both return the same tensor\n",
            "SUCCESS: both return exception\n",
            "SUCCESS: both return exception\n",
            "SUCCESS: both return the same tensor\n",
            "SUCCESS: both return the same tensor\n",
            "\n",
            "\n",
            "TEST: broadcastible_together\n",
            "\n",
            "SUCCESS: both return the same result\n",
            "SUCCESS: both return the same result\n",
            "SUCCESS: both return the same bool (FALSE)\n",
            "SUCCESS: both return the same result\n",
            "SUCCESS: both return the same result\n",
            "SUCCESS: both return the same result\n",
            "SUCCESS: both return the same result\n",
            "SUCCESS: both return the same result\n",
            "SUCCESS: both return the same result\n",
            "SUCCESS: both return the same result\n",
            "SUCCESS: both return the same result\n",
            "\n",
            "\n",
            "TEST: broadcast_together\n",
            "\n",
            "SUCCESS: both return the same result\n",
            "SUCCESS: both return the same result\n",
            "SUCCESS: both return exception\n",
            "SUCCESS: both return the same result\n",
            "SUCCESS: both return the same result\n",
            "SUCCESS: both return the same result\n",
            "SUCCESS: both return the same result\n",
            "SUCCESS: both return the same result\n",
            "SUCCESS: both return the same result\n",
            "SUCCESS: both return the same result\n",
            "SUCCESS: both return the same result\n",
            "SUCCESS: both return the same result\n"
          ]
        }
      ]
    }
  ]
}